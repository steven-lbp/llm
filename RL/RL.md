## PPO vs GRPO
### 本质区别：**baseline 的来源**

- PPO：

使用单独的 **Value Model $V(s)$** 预测 baseline → 更通用，但训练复杂，易引入估值误差。

$$
A = R - V(s)
$$

- GRPO：

用同一 prompt 下的多个回答的 **组内平均得分 $\mu_G$** 作为 baseline → 更轻量、更稳定。

$$
A_i^G = \frac{R(o_i) - \mu_G}{\sigma_G + \epsilon}
$$

### 核心直觉

| 场景      | PPO 的做法                                | GRPO 的做法                |
| ------- | -------------------------------------- | ----------------------- |
| 老师问一个问题 | 预测“这题你大概能考 80 分”，然后比较你实际得了多少（预测 vs 实际） | 直接看这一组同学谁考得高，奖励组内比别人高的人 |
| 要训练的    | Value 模型 + 策略模型                        | 只训练策略模型                 |
| 学习信号    | $R - V$：你表现比预期好                        | $R - \mu$：你在组里表现比别人好    |


### 为什么 GRPO 更适合语言大模型？

1. **语言模型的“状态”复杂**：不像游戏环境那样有明确状态，Value Model 很难准确估计 $V(s)$
2. **直接从输出分数计算相对表现更简单有效**
3. **减少训练结构复杂性**，只训练一个策略模型
4. **组内比较自然支持排序学习**，能强化相对优质的生成内容（例如回答、摘要）

## GRPO

$$
 \mathcal{L}_{GRPO}(\theta) = - \frac{1}{G} \sum_{i=1}^{G} \log \pi_\theta(o_i | q) \cdot A_i^G + \beta \cdot D_{KL} (\pi_{\theta_{old}} \parallel \pi_\theta)
$$

### 群体相对优势

$$
 A_i^G = \frac{R(o_i) - \mu_G}{\sigma_G + \epsilon}
$$

**含义**：

* $R(o_i)$：对每个回答打分（来自奖励模型、打标签或规则评估）
* $\mu_G$：该 prompt 下，组内回答的平均分
* $\sigma_G$：组内得分标准差
* $A_i^G$：表示这个回答相对其他人**好多少**

**总结**：这是一个“标准分（Z-score）”，表示**在组内的位置优势**

---
### 对数概率

$$
\log \pi_\theta(o_i \mid q)
$$
模型在参数为 $\theta$ 的状态下，输出 $o_i$ 这个回答的**对数概率**。

* $\pi_\theta$：表示模型（就是你的大语言模型）
* $o_i$：模型输出的一段文字（如一个回答）
* $q$：是输入的 prompt（比如一个问题）

> **为什么用 log？**
> log 概率更稳定、更适合做梯度计算（机器学习中的常规技巧）

---
### 核心目标项：组内损失

$$
 \frac{1}{G} \sum_{i=1}^{G} \log \pi_\theta(o_i | q) \cdot A_i^G
$$

**逐层解释**：

* $\log \pi_\theta(o_i | q)$：模型生成第 $i$ 个回答的 log 概率
* 乘上优势 $A_i^G$：

  * 如果该回答比别人好（$A > 0$），就要**提升它的概率**
  * 如果该回答拉胯（$A < 0$），就要**降低它的概率**
* 对所有样本求平均 $\frac{1}{G} \sum$：整组一起学习，提升“组整体质量”
* 负号：因为我们用 **梯度下降（最小化）**，但希望目标是 **最大化优势**

**总结**：这是让模型在每组中，**更偏向于生成那些“表现好”的回答**

---

### 控制项：KL 散度惩罚

$$
 \beta \cdot D_{KL}(\pi_{\theta_{old}} \parallel \pi_\theta)
$$

**含义**：

* $\pi_{\theta_{\text{old}}}$：旧版本模型（上一步或SFT模型）
* $\pi_\theta$：当前模型
* $D_{\text{KL}}$：衡量两个分布差距（越小越相似）
* $\beta$：系数（控制惩罚强度）

**作用**：防止模型跑偏，生成风格“失控”。

* 比如，如果模型突然学会用一些奇怪的方式回答，只为了骗过奖励模型，这就不行。
* KL 惩罚确保模型**不会变得太不一样**，保持稳定性和“原有风格”。

**总结**：KL 项是一个“约束绳索”，防止模型发散，确保可控、平稳地学习。
