## GRPO
$$
 \mathcal{L}_{\text{GRPO}}(\theta) = - \frac{1}{G} \sum_{i=1}^{G} \log \pi_\theta(o_i | q) \cdot A_i^G + \beta \cdot D_{\text{KL}}(\pi_{\theta_{\text{old}}} \parallel \pi_\theta)
$$

---
### 群体相对优势
$$
 A_i^G = \frac{R(o_i) - \mu_G}{\sigma_G + \epsilon}
$$

**含义**：

* $R(o_i)$：对每个回答打分（来自奖励模型、打标签或规则评估）
* $\mu_G$：该 prompt 下，组内回答的平均分
* $\sigma_G$：组内得分标准差
* $A_i^G$：表示这个回答相对其他人**好多少**

**总结**：这是一个“标准分（Z-score）”，表示**在组内的位置优势**

---
### 对数概率
$$
\log \pi_\theta(o_i \mid q)
$$
模型在参数为 $\theta$ 的状态下，输出 $o_i$ 这个回答的**对数概率**。

* $\pi_\theta$：表示模型（就是你的大语言模型）
* $o_i$：模型输出的一段文字（如一个回答）
* $q$：是输入的 prompt（比如一个问题）

> **为什么用 log？**
> log 概率更稳定、更适合做梯度计算（机器学习中的常规技巧）

---
### 核心目标项：组内损失

$$
 \frac{1}{G} \sum_{i=1}^{G} \log \pi_\theta(o_i | q) \cdot A_i^G
$$

**逐层解释**：

* $\log \pi_\theta(o_i | q)$：模型生成第 $i$ 个回答的 log 概率
* 乘上优势 $A_i^G$：

  * 如果该回答比别人好（$A > 0$），就要**提升它的概率**
  * 如果该回答拉胯（$A < 0$），就要**降低它的概率**
* 对所有样本求平均 $\frac{1}{G} \sum$：整组一起学习，提升“组整体质量”
* 负号：因为我们用 **梯度下降（最小化）**，但希望目标是 **最大化优势**

**总结**：这是让模型在每组中，**更偏向于生成那些“表现好”的回答**

---

### 控制项：KL 散度惩罚

$$
+ \beta \cdot D_{\text{KL}}(\pi_{\theta_{\text{old}}} \parallel \pi_\theta)
$$

**含义**：

* $\pi_{\theta_{\text{old}}}$：旧版本模型（上一步或SFT模型）
* $\pi_\theta$：当前模型
* $D_{\text{KL}}$：衡量两个分布差距（越小越相似）
* $\beta$：系数（控制惩罚强度）

**作用**：防止模型跑偏，生成风格“失控”。

* 比如，如果模型突然学会用一些奇怪的方式回答，只为了骗过奖励模型，这就不行。
* KL 惩罚确保模型**不会变得太不一样**，保持稳定性和“原有风格”。

**总结**：KL 项是一个“约束绳索”，防止模型发散，确保可控、平稳地学习。

